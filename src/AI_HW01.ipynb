{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz5PeiNYChbP"
   },
   "source": [
    "# HW01 Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDVL8iTxChbS"
   },
   "source": [
    "## Q1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) When a slight reward adjustment of \\(c\\) is introduced to all transitions in the MDP, the new optimal value function, denoted as $(V_{1,c})$, can be expressed as:\n",
    "\n",
    "${ V_{1,c}(s) = c \\cdot \\sum_{s'} P(s' \\,|\\, s, \\pi_1(s))[R(s, \\pi_1(s), s') + \\gamma \\cdot V_{1,c}(s')] }$\n",
    "\n",
    "The optimal policy ${\\pi_1}$ may or may not change, depending on the specific nature of the MDP and the magnitude and sign of the reward adjustment c.\n",
    "\n",
    "b) If all rewards are scaled by an arbitrary real constant $c \\in \\mathbb{R}$, the new optimal value function $V_{1,c}$ can be expressed as:\n",
    "\n",
    "$ V\\_{1,c}(s) = c \\cdot V_1(s) $\n",
    "\n",
    "The optimal policy $\\pi_1$ remains unchanged when scaling rewards, as the policy is determined by the relative values and remains optimal for the scaled rewards. Therefore, $\\pi_1$ remains optimal for any choice of $c$.\n",
    "\n",
    "c) The presence of terminal states does affect the response to part (a). If there are terminal states that end an episode, the response to part (a) changes. The introduction of terminal states creates a finite horizon problem, and the optimal value function becomes time-dependent. The new optimal value function is denoted as $V_{1,c,T}$, and it satisfies the following expression:\n",
    "\n",
    "$ V*{1,c,T}(s) = c \\cdot \\sum*{s'} P(s' \\,|\\, s, \\pi*1(s))[R(s, \\pi_1(s), s') + \\gamma \\cdot V*{1,c,T}(s')], \\quad s \\notin T $\n",
    "\n",
    "where $T$ is the set of terminal states.\n",
    "\n",
    "An example where responses to part (a) and part (c) would diverge is an MDP with terminal states where the optimal policy for the original MDP $\\pi_1$ might not be optimal in the modified MDP with terminal states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST3brl2CChbS"
   },
   "source": [
    "## Q2 Pacman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRSKYOWAChbS"
   },
   "source": [
    "### import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uMPqNJQRChbT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display class for optional section:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class display:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def Animation(board):\n",
    "        WIDTH, HEIGHT = 60, 50\n",
    "        map_data = np.array(board)\n",
    "        window_size = (WIDTH * map_data.shape[1], HEIGHT * map_data.shape[0])\n",
    "        screen = pygame.display.set_mode(window_size)\n",
    "        screen.fill((0, 0, 0))\n",
    "        WHITE = (255, 255, 255)\n",
    "        BRWON = (150, 75, 0)\n",
    "        GREEN = (0, 128, 0)\n",
    "        \n",
    "        for y, row in enumerate(map_data):\n",
    "            for x, cell in enumerate(row):\n",
    "                rect = pygame.Rect(x * WIDTH, y * HEIGHT, WIDTH, HEIGHT)\n",
    "                if cell == 'W':\n",
    "                    pygame.draw.rect(screen, BRWON, rect)\n",
    "                elif cell == 'A':\n",
    "                    pygame.draw.rect(screen, GREEN, rect)\n",
    "                elif cell == 'D':\n",
    "                    pygame.draw.circle(screen, WHITE, (x * WIDTH + WIDTH // 2, y * HEIGHT + HEIGHT // 2), 10)\n",
    "        pygame.display.flip()\n",
    "        pygame.display.update()\n",
    "        pygame.time.delay(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "10XCjM8DChbT"
   },
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__(self) -> None:\n",
    "        self.initilaBoard = [\n",
    "            ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W'],\n",
    "            ['W', 'A', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'W'],\n",
    "            ['W', 'D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D', 'W'],\n",
    "            ['W', 'D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D', 'W'],\n",
    "            ['W', 'D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D', 'W'],\n",
    "            ['W', 'D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D', 'W'],\n",
    "            ['W', 'D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D', 'W'],\n",
    "            ['W', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'W'],\n",
    "            ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W']\n",
    "        ]\n",
    "        self.qTable = np.zeros((3**8, 4)) \n",
    "        self.currentBoard = []\n",
    "\n",
    "    def makeNewBoard(self) -> list:\n",
    "        return [i.copy() for i in self.initilaBoard]   \n",
    "\n",
    "    def getBoardRowsNumber()->int:\n",
    "        return 7\n",
    "\n",
    "    def getBoardColumnsNumber()->int:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xpuSkwgXChbU"
   },
   "outputs": [],
   "source": [
    "class QLearning(Board):\n",
    "    \n",
    "    alpha = 0.7\n",
    "    gamma = 0.8\n",
    "    epcilon = 0.9\n",
    "    \n",
    "    WALL_REWARD = -2\n",
    "    EMPTHY_REWARD = -1\n",
    "    DOT_REWARD = 2\n",
    "    REWARD_MAPPER = {\n",
    "        \"W\" : WALL_REWARD , \n",
    "        \"E\" : EMPTHY_REWARD, \n",
    "        \"D\" : DOT_REWARD,\n",
    "    }\n",
    "\n",
    "    HASH_MAPPER = {\n",
    "        \"W\" : 0, \n",
    "        \"E\" : 1, \n",
    "        \"D\" : 2\n",
    "    }\n",
    "\n",
    "    ACTION_MAPPER = {\n",
    "        0 : \"UP\", \n",
    "        1 : \"DOWN\", \n",
    "        2: \"RIGHT\", \n",
    "        3:\"LEFT\",\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.preResult = 0\n",
    "        super().__init__()\n",
    "\n",
    "    def QFunction(self,q :int, qMax:int, reward:int):\n",
    "        return q + self.alpha * (reward + self.gamma * qMax - q)\n",
    "\n",
    "    def qTableHash(self, i:int , j:int)->int:\n",
    "        iter = 0\n",
    "        result = 0\n",
    "        for m in range(-1, 2):\n",
    "            for n in range(-1, 2):\n",
    "                if m!= 0 or n !=0:\n",
    "                    result += (self.HASH_MAPPER[self.currentBoard[i+m][j+n]] * (3**iter))\n",
    "                    iter += 1\n",
    "        return result\n",
    "\n",
    "    def getStateAndAction(self, i: int, j: int):\n",
    "        if random.random() < self.epcilon:\n",
    "            return self.qTableHash(i, j) ,random.randint(0,3)\n",
    "        return self.qTableHash(i, j), np.argmax(self.qTable[self.qTableHash(i, j)])\n",
    "    \n",
    "    def testGetStateAndAction(self, i:int, j:int): \n",
    "        if (self.qTableHash(i,j) == self.preResult): #huristic\n",
    "            tmp = []\n",
    "            if(self.currentBoard[i+1][j]!=\"W\") : tmp.append(1)\n",
    "            if(self.currentBoard[i-1][j]!=\"W\") : tmp.append(0)\n",
    "            if(self.currentBoard[i][j+1]!=\"W\") : tmp.append(2)\n",
    "            if(self.currentBoard[i][j-1]!=\"W\") : tmp.append(3)\n",
    "            return self.qTableHash(i, j), random.choice(tmp)\n",
    "        self.preResult =  self.qTableHash(i, j)\n",
    "        return self.qTableHash(i, j), np.argmax(self.qTable[self.qTableHash(i, j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ga2ujTGEChbU"
   },
   "outputs": [],
   "source": [
    "class PackMan(QLearning):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def resetActionPosition(self)->None:\n",
    "        self.i= 1\n",
    "        self.j = 1 \n",
    "\n",
    "    def calculateNumberOfDots(self)->int:\n",
    "        return sum(row.count('D') for row in self.initilaBoard) \n",
    "\n",
    "    def run(self, mode = \"train\", displayAnime = False):\n",
    "        self.resetActionPosition()\n",
    "        self.numberOfDots = self.calculateNumberOfDots()\n",
    "\n",
    "        if(mode == \"test\" and displayAnime):\n",
    "            pygame.init()\n",
    "            display.Animation(self.currentBoard)\n",
    "\n",
    "        while self.numberOfDots > 0 :\n",
    "        \n",
    "            if mode == \"train\":\n",
    "                currentState, action = self.getStateAndAction(self.i, self.j)\n",
    "            else:\n",
    "                currentState, action = self.testGetStateAndAction(self.i, self.j)  \n",
    "            reward = 0\n",
    "            \n",
    "            if self.ACTION_MAPPER[action] == \"UP\":\n",
    "                if self.currentBoard[self.i-1][self.j] != \"W\":\n",
    "                    reward = self.REWARD_MAPPER[self.currentBoard[self.i-1][self.j]]\n",
    "                    self.currentBoard[self.i-1][self.j] = \"A\"\n",
    "                    self.currentBoard[self.i][self.j] = \"E\"\n",
    "                    self.i -=1    \n",
    "            elif self.ACTION_MAPPER[action] == \"DOWN\":\n",
    "                if self.currentBoard[self.i+1][self.j] != \"W\":\n",
    "                    reward = self.REWARD_MAPPER[self.currentBoard[self.i+1][self.j]]\n",
    "                    self.currentBoard[self.i+1][self.j] = \"A\"\n",
    "                    self.currentBoard[self.i][self.j] = \"E\"\n",
    "                    self.i +=1         \n",
    "            elif self.ACTION_MAPPER[action] == \"RIGHT\":\n",
    "                if self.currentBoard[self.i][self.j+1] != \"W\":\n",
    "                    reward = self.REWARD_MAPPER[self.currentBoard[self.i][self.j+1]]\n",
    "                    self.currentBoard[self.i][self.j+1] = \"A\"\n",
    "                    self.currentBoard[self.i][self.j] = \"E\"\n",
    "                    self.j +=1 \n",
    "            elif self.ACTION_MAPPER[action] == \"LEFT\":\n",
    "                if self.currentBoard[self.i][self.j-1] != \"W\":\n",
    "                    reward = self.REWARD_MAPPER[self.currentBoard[self.i][self.j-1]]\n",
    "                    self.currentBoard[self.i][self.j-1] = \"A\"\n",
    "                    self.currentBoard[self.i][self.j] = \"E\"\n",
    "                    self.j -=1\n",
    "\n",
    "            if reward == self.DOT_REWARD:\n",
    "                self.numberOfDots -=1\n",
    "\n",
    "            nextState = self.qTableHash(self.i, self.j)\n",
    "            if mode == \"train\":\n",
    "                self.qTable[currentState][action] = self.QFunction(self.qTable[currentState][action], max(self.qTable[nextState]), reward)\n",
    "                \n",
    "            if mode == \"test\" and displayAnime: \n",
    "                display.Animation(self.currentBoard)\n",
    "                       \n",
    "    def setAndChangeParams(self, alpha, gamma, epsilon)->None:\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epcilon = epsilon\n",
    "        \n",
    "    def train(self):\n",
    "        n = 80\n",
    "        for i in range(n):\n",
    "            self.currentBoard = self.makeNewBoard() \n",
    "            if i < 40:\n",
    "                self.epcilon = self.epcilon * ((n-(i%25))/ n)\n",
    "            self.run()  \n",
    "        print(\"done\")\n",
    "        \n",
    "    def test(self, displayAnime = True):\n",
    "        self.resetActionPosition()\n",
    "        self.currentBoard = self.makeNewBoard()\n",
    "        self.run(\"test\", displayAnime)\n",
    "        print(np.array(self.currentBoard))\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kwIQcPBEChbV",
    "outputId": "4637119e-2744-4732-ea4d-5798e313114b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "packMan = PackMan()\n",
    "packMan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'W' 'W' 'E' 'W' 'W' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'E' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'W' 'E' 'W' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'A' 'W' 'E' 'W' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'W' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']]\n"
     ]
    }
   ],
   "source": [
    "packMan.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please check pdf file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in my code, I definde my action like this:\n",
    "\n",
    "```py\n",
    "ACTION_MAPPER = {\n",
    "    0 : \"UP\",\n",
    "    1 : \"DOWN\",\n",
    "    2: \"RIGHT\",\n",
    "    3:\"LEFT\",\n",
    "}\n",
    "```\n",
    "\n",
    "and as I said in the first question I have a hash function that defined my states, every state is hash of 8 adjacent element of that.\n",
    "and as you can see in code my rewards are:\n",
    "\n",
    "```py\n",
    "WALL_REWARD = -2\n",
    "EMPTHY_REWARD = -1\n",
    "DOT_REWARD = 2\n",
    "REWARD_MAPPER = {\n",
    "    \"W\" : WALL_REWARD ,\n",
    "    \"E\" : EMPTHY_REWARD,\n",
    "    \"D\" : DOT_REWARD,\n",
    "}\n",
    "```\n",
    "\n",
    "and finally my goal is to decrease dots, and when number of dots reached to 0 I will terminate the game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "for gamma =  0.25  train duration time :  42.10186696052551\n",
      "[['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'W' 'W' 'E' 'W' 'W' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'E' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'W' 'E' 'W' 'E' 'A' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'W' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']]\n",
      "for gamma =  0.25  test duration time :  0.010433197021484375\n",
      "done\n",
      "for gamma =  0.5  train duration time :  48.255850315093994\n",
      "[['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'A' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'W' 'W' 'E' 'W' 'W' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'E' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'W' 'E' 'W' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'W' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']]\n",
      "for gamma =  0.5  test duration time :  0.0028297901153564453\n",
      "done\n",
      "for gamma =  0.99  train duration time :  2.1547820568084717\n",
      "[['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'W' 'W' 'E' 'W' 'W' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'E' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'W' 'E' 'W' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'W' 'E' 'A' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']]\n",
      "for gamma =  0.99  test duration time :  0.0016417503356933594\n"
     ]
    }
   ],
   "source": [
    "for gamma in [0.25, 0.5, 0.99]:\n",
    "    packMan = PackMan()\n",
    "    packMan.setAndChangeParams(packMan.alpha, gamma, 0.9)\n",
    "    start = time.time()\n",
    "    packMan.train()\n",
    "    end = time.time()\n",
    "    print(\"for gamma = \" , gamma , \" train duration time : \", end - start)\n",
    "    start = time.time()\n",
    "    packMan.test(False)\n",
    "    end = time.time()\n",
    "    print(\"for gamma = \" , gamma , \" test duration time : \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma is a value of the future reward and as you can see for this problem when we increase the gamma the impact of next states will remove and the current state reward will be our reward, so we don't learn the match and we will just faced to overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "for alpha =  0.99  train duration time :  58.32253313064575\n",
      "[['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'W' 'W' 'E' 'W' 'W' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'E' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'W' 'E' 'W' 'E' 'A' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'W' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']]\n",
      "for alpha =  0.99  test duration time :  0.00392460823059082\n",
      "done\n",
      "for alpha =  0.6  train duration time :  48.06031537055969\n",
      "[['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'W' 'W' 'E' 'W' 'W' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'E' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'A' 'E' 'W' 'E' 'W' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'W' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']]\n",
      "for alpha =  0.6  test duration time :  0.012758493423461914\n",
      "done\n",
      "for alpha =  0.1  train duration time :  50.95813226699829\n",
      "[['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'W' 'W' 'E' 'W' 'W' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'E' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'W' 'E' 'W' 'E' 'E' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'W' 'E' 'E' 'W' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'E' 'E' 'E' 'A' 'E' 'E' 'W']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']]\n",
      "for alpha =  0.1  test duration time :  0.00458979606628418\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.99, 0.6, 0.1]:\n",
    "    packMan = PackMan()\n",
    "    packMan.setAndChangeParams(alpha, packMan.gamma, 0.9)\n",
    "    start = time.time()\n",
    "    packMan.train()\n",
    "    end = time.time()\n",
    "    print(\"for alpha = \" , alpha , \" train duration time : \", end - start)\n",
    "    start = time.time()\n",
    "    packMan.test(False)\n",
    "    end = time.time()\n",
    "    print(\"for alpha = \" , alpha , \" test duration time : \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha is the learning rate, as you can see when we decrease it to 0 it means q table values don't have many changes but if we increase it to 0.6 we can see this changes and if alpha converge to 1 we can see that learning will occur quickly and q is not very important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  0.  0. -1.]\n",
      "[ 6.05169454  1.45824     1.45824    -0.91      ]\n",
      "[-0.973       1.99371063  0.          7.70888568]\n",
      "[-1.e+000  1.e-323 -1.e+000  1.e-323]\n",
      "[ 3.07356  0.      -0.7      0.784  ]\n",
      "[ 1.e-323  1.e-323 -1.e+000 -1.e+000]\n",
      "[-1.e+000  1.e-323 -1.e+000 -1.e+000]\n",
      "[-0.7     0.      0.      0.5649]\n",
      "[ 2.          0.         -0.38185442  0.        ]\n",
      "[1.4 0.  0.  0. ]\n",
      "[ 0.          0.         -0.99039227 -1.        ]\n",
      "[ 0.45763096  0.45763096 -0.79988343 -0.77541772]\n",
      "[-1.          0.         -1.         -0.99594852]\n",
      "[ 2.      1.0192  0.     -0.7   ]\n",
      "[ 0.          0.          0.59700566 -0.07316744]\n",
      "[ 0.   0.   0.  -0.7]\n",
      "[ 1.8463517   1.96372064 -0.68798049  2.0032352 ]\n",
      "[-0.7         0.         -0.99757     5.61377639]\n",
      "[-0.7         0.         -0.7         5.87416923]\n",
      "[ 6.7232      2.7056556  -0.12499556  0.        ]\n",
      "[0.         0.         0.         1.98384401]\n",
      "[ 1.84167687  2.83430315 -0.64757466  7.35501821]\n",
      "[ 0.71833886  0.784      -0.81051967  7.22436737]\n",
      "[0.         0.         0.         4.87586347]\n",
      "[0.         0.         0.         3.59974165]\n",
      "[-0.7         5.49755528  6.15795367  4.49868594]\n",
      "[ 2.4383699   2.21139262  9.13533949 -0.83554337]\n",
      "[ 1.08086002  1.966272    8.7226473  -0.973     ]\n",
      "[6.70627629 0.         0.         0.        ]\n",
      "[2.604 0.    1.4   0.   ]\n",
      "[1.0192 1.2544 1.946  0.    ]\n",
      "[ 0.         0.784      4.5693657 -0.7      ]\n",
      "[ 2.97445288  2.62013976  5.02534015 -0.79277536]\n",
      "[0.        0.        3.5632128 0.       ]\n",
      "[-0.9919      0.          6.82037984 -0.91      ]\n",
      "[9.7060751  3.71049278 3.63944    0.        ]\n",
      "[0.         0.         2.38347201 1.4       ]\n",
      "[ 1.e-323  1.e-323 -1.e+000 -1.e+000]\n",
      "[ 5.29506454  4.95240124  9.08793018 -0.8210909 ]\n",
      "[ 0.    0.    0.   -0.91]\n",
      "[ 1.45824     0.         -0.96068966  3.44988   ]\n",
      "[2.27470157 1.85207787 1.4        3.16673304]\n",
      "[-1.0000000e+000 -5.6040025e-001  9.8813129e-324  9.8813129e-324]\n",
      "[ 0.8172213  -0.84391533  0.          0.        ]\n",
      "[ 5.99706924 -0.89827163  3.03458415  2.6400555 ]\n",
      "[-1.         -0.99996928  0.          0.        ]\n",
      "[-0.30337946 -1.          0.          0.        ]\n",
      "[-0.7  0.   0.   0. ]\n",
      "[5.67228076 1.45259431 1.37697936 0.11131702]\n",
      "[ 1.96394097 -0.973       0.          1.01177191]\n",
      "[0.58218406 0.         0.         0.        ]\n",
      "[ 0.  -0.7  0.   0. ]\n",
      "[ 1.e-323 -1.e+000  1.e-323 -1.e+000]\n",
      "[-1.00000000e+000 -1.00000000e+000  9.88131292e-324 -7.83976508e-001]\n",
      "[6.30226179 0.78398057 4.23167332 0.43614796]\n",
      "[-1.         -0.97742969  0.         -0.99113827]\n",
      "[1.4 0.  0.  0. ]\n",
      "[0.1180074 0.        0.        0.       ]\n",
      "[5.3946641 0.        0.        0.       ]\n",
      "[ 2.77022586 -0.91        2.85049244  5.2781341 ]\n",
      "[0.01257256 0.17378276 0.         2.00004867]\n",
      "[0.   0.   0.   1.82]\n",
      "[ 0.65315308 -0.65296     1.4230384   2.86151121]\n",
      "[ 1.4 -0.7  0.   1.4]\n",
      "[ 0.73215852 -0.7         0.          0.        ]\n",
      "[9.59136976 0.         0.         0.        ]\n",
      "[ 1.e-323 -1.e+000 -1.e+000  1.e-323]\n",
      "[-0.99996216 -1.         -1.          0.        ]\n",
      "[-1.          3.56596074 -0.99996178  0.        ]\n",
      "[ 0.    -0.973 -0.91   0.   ]\n",
      "[1.4 0.  0.  0. ]\n",
      "[ 1.e-323 -1.e+000 -1.e+000 -1.e+000]\n",
      "[-1. -1. -1. -1.]\n",
      "[ 1.660512    0.         -0.97915378  5.00600629]\n",
      "[-0.973     -0.7       -0.91       3.8853024]\n",
      "[4.12374084 0.         0.         0.        ]\n",
      "[0.         2.11982613 7.35537648 1.22304   ]\n",
      "[-0.7       -0.7805616  2.0012469  0.784    ]\n",
      "[-0.42711829 -0.43760186  2.00491675  0.        ]\n",
      "[0.56784 0.      0.      0.     ]\n",
      "[ 3.62019887 -0.7         0.          0.        ]\n",
      "[3.637024   0.75824    8.0119383  0.03704693]\n",
      "[-0.16697782 -0.41200656  4.196094   -0.7       ]\n",
      "[ 8.51316471 -0.07701752  1.4         0.        ]\n",
      "[ 3.04075094 -0.9919      8.49004901  4.97155515]\n",
      "[ 0.        -0.86296    3.369968   2.2446144]\n",
      "[-1.e+000 -1.e+000  1.e-323  1.e-323]\n",
      "[ 3.27115922 -0.7         2.256744    0.        ]\n",
      "[-0.9997767 -1.         0.         0.       ]\n",
      "[1.4 0.  0.  0. ]\n",
      "[ 0.63337509 -0.91        0.52429503  0.        ]\n",
      "[4.73517636 0.         0.         1.5288    ]\n",
      "[ 0. -1. -1.  0.]\n",
      "[ 1.11918352 -0.91067116  7.7746597   1.11997796]\n",
      "[-0.895888   0.6008862  0.         0.       ]\n",
      "[-5.95666988e-01  5.72235427e+00  8.77214780e-23  8.83613373e-23]\n",
      "[4.88       0.084      0.         2.65895043]\n",
      "[0.         0.57488132 0.13740343 0.        ]\n",
      "[ 1.43215852 -0.7         9.41514591  0.        ]\n",
      "[1.27591359 8.10816957 3.73007674 5.98215028]\n",
      "[0.         9.13349371 0.         0.        ]\n",
      "[-0.52239525  6.38426403  2.64816384  3.30405049]\n",
      "[0.62057615 9.3574559  3.21347161 6.86461514]\n",
      "[ 0.          8.27384099  3.50787476 -0.7       ]\n",
      "[-0.41269843  4.47615627  0.          0.        ]\n",
      "[ 0.          6.27692922  3.32791973 -0.2753856 ]\n",
      "[-0.7  1.4  0.   0. ]\n",
      "[ 0.     1.82   0.784 -0.7  ]\n",
      "[ 4.71514396  2.184       0.         -0.7       ]\n",
      "[-0.7         9.26744602  3.58650545  1.82      ]\n",
      "[0.784      5.71749987 1.47998298 2.83185065]\n",
      "[-0.7         5.51326483  1.24912095  3.58198579]\n",
      "[0.        4.2876624 0.        0.       ]\n",
      "[-0.7         2.9288     -0.36835972  0.        ]\n",
      "[1.4 0.  0.  0. ]\n",
      "[3.2382 0.     0.     1.0192]\n",
      "[ 0.          3.13361518 -0.7         0.        ]\n",
      "[1.660512   9.72096061 3.77835355 3.62600981]\n",
      "[0.19776763 9.3748476  5.51690331 0.        ]\n",
      "[-0.5263608   2.604       4.50582268  1.69344   ]\n",
      "[0.       2.491832 0.       0.      ]\n",
      "[-0.405697    8.232193    4.11101554  3.3260631 ]\n",
      "[-0.7        4.2684264  1.110928   1.0192   ]\n",
      "[-0.7  1.4  0.   0. ]\n",
      "[ 0.          3.47149858 -0.7         0.        ]\n",
      "[ 1.e-323  1.e-323 -1.e+000 -1.e+000]\n",
      "[ 1.0192      2.92177651 -0.78227006  3.6316079 ]\n",
      "[ 0.   0.   1.4 -0.7]\n",
      "[-1.  0.  0.  0.]\n",
      "[ 0.          0.         -0.46525138  0.22969829]\n",
      "[ 0.          0.         -1.66739629 -1.        ]\n",
      "[ 0.          0.         -0.98715265 -0.99999968]\n",
      "[ 0.          0.          3.24965458 -0.7       ]\n",
      "[0.         0.         1.75051968 0.        ]\n",
      "[ 0.   0.  -0.7  0. ]\n",
      "[0.         0.         0.02997227 2.604     ]\n",
      "[0.  0.  0.  1.4]\n",
      "[ 0.          0.          1.4        -1.72556853]\n",
      "[ 1.45824     5.05078044  9.15977978 -0.58319177]\n",
      "[ 0.         0.         3.0006144 -0.9919   ]\n",
      "[0.      0.      0.      0.52304]\n",
      "[ 0.29828878  0.33770897 -0.79426898 -0.57962262]\n",
      "[1.7598801  0.         2.41741772 0.75824   ]\n",
      "[ 0.         0.        -1.785714   4.0894999]\n",
      "[ 0.   0.  -0.7  1.4]\n",
      "[ 1.5288  1.764  -0.7     3.2382]\n",
      "[-1.         -0.99999973  0.          0.        ]\n",
      "[-0.98922624 -1.          0.          0.        ]\n",
      "[ 1.15417296 -0.83961535  0.          0.0404544 ]\n",
      "[ 1.946      -0.64348259  0.784       0.        ]\n",
      "[3.709748   1.15147629 0.         2.62486336]\n",
      "[ 9.88131292e-324 -1.00000000e+000  9.88131292e-324 -9.89591053e-001]\n",
      "[ 0.        -0.91       0.         1.9722156]\n",
      "[-1.  -1.8  0.   0. ]\n",
      "[ 3.16478639 -0.33787112  0.          0.        ]\n",
      "[-0.7  0.   0.   0. ]\n",
      "[-0.7 -0.7  0.   0. ]\n",
      "[ 2.184      -1.63799988  0.          0.        ]\n",
      "[-0.44857792 -1.          0.         -1.        ]\n",
      "[1.4 0.  0.  0. ]\n",
      "[ 0. -1.  0. -1.]\n",
      "[-0.64815479 -0.7         0.          0.        ]\n",
      "[ 1.4 -0.7  0.   0. ]\n",
      "[-9.99971441e-001 -1.00000000e+000 -1.00000000e+000  9.88131292e-324]\n",
      "[ 0. -1. -1.  0.]\n",
      "[ 0.        -0.91       0.6166123  0.       ]\n",
      "[-0.99659596 -0.189       0.          0.        ]\n",
      "[0.3192 0.     0.     0.    ]\n",
      "[2.01382294 0.         0.         0.        ]\n",
      "[6.93478469 0.         0.         0.        ]\n",
      "[-0.91        1.49157734  0.         -0.7       ]\n",
      "[ 0.    -0.973  0.     0.   ]\n",
      "[0.        0.        0.        0.6942656]\n",
      "[-0.41160631 -0.126       0.          8.38774567]\n",
      "[5.904 0.    0.    0.   ]\n",
      "[0.      0.      0.      3.04304]\n",
      "[0.         0.         0.         5.92901653]\n",
      "[-0.9919      2.15077369 -0.91        0.        ]\n",
      "[2.454032 0.       0.       0.      ]\n",
      "[ 0.          3.19223839 -0.7         0.        ]\n",
      "[-0.69992734  0.84        0.          0.        ]\n",
      "[-0.90928899  3.51344     0.          0.        ]\n",
      "[2.97519103 0.         0.         1.14981518]\n",
      "[-0.7  1.4  0.   0. ]\n",
      "[0.         0.         0.         4.63300006]\n",
      "[-0.973       4.35885549 -0.91        0.        ]\n",
      "[ 0.          7.40414158 -0.7         0.        ]\n",
      "[ 0.         7.8600198 -0.91       1.3942656]\n",
      "[0.    0.    0.084 0.   ]\n",
      "[ 2.85647087  4.80882507  8.76355422 -0.78597568]\n",
      "[ 0.          0.          0.09219    -1.25931329]\n",
      "[ 0.    0.   -0.91 -0.7 ]\n",
      "[ 2.0149584   0.784       4.79834398 -0.91      ]\n",
      "[0.        0.        3.4380864 0.       ]\n",
      "[3.13670872 0.         6.63639289 0.        ]\n",
      "[-0.41200656  0.          0.          0.        ]\n",
      "[ 0.   0.  -0.7 -0.7]\n",
      "[0.         0.         1.99936755 0.        ]\n",
      "[0.     0.     0.     2.9288]\n",
      "[2.89608697 3.96570276 5.6823197  1.83875005]\n",
      "[-0.85738141  0.61190863  0.56627062  0.49837312]\n",
      "[-0.7         6.6688077   0.          3.61068152]\n",
      "[-0.7  0.   0.   0. ]\n",
      "[9.65602763 4.48769699 2.20342522 0.061152  ]\n",
      "[ 0.23263632  0.95515363  0.         -0.973     ]\n",
      "[ 0.        -0.86296    0.         2.0848064]\n",
      "[-0.25438372  1.68739021  1.10524331  0.79513907]\n",
      "[1.22532838 0.         0.         0.        ]\n",
      "[0.54824   0.9589328 0.        0.       ]\n",
      "[8.06340573 0.084      3.69183942 1.08976   ]\n",
      "[-0.7         2.39332768  0.         -0.7       ]\n",
      "[ 2.25176     0.1092      0.08627136 -0.91      ]\n",
      "[ 0.          6.5435122   2.37114979 -0.62059656]\n",
      "[0.    0.    0.784 1.82 ]\n",
      "[-0.7        -0.7         3.35151512  0.        ]\n",
      "[4.64669654 0.         0.         0.        ]\n",
      "[ 0.         -0.189       9.49197558  2.22130406]\n",
      "[9.65840016 0.         1.946      0.        ]\n",
      "[0.         0.48524    5.46093257 0.784     ]\n",
      "[0.36533168 7.82831018 5.59058587 2.3487072 ]\n",
      "[-0.91  1.4   0.    0.  ]\n",
      "[ 1.764       5.31298366  1.0192     -0.9844933 ]\n",
      "[2.5052962 0.        0.        0.       ]\n",
      "[-0.7         6.40709106  0.          0.        ]\n",
      "[2.88122159 0.         0.         0.        ]\n",
      "[ 5.45324981  9.1364254   4.22193466 -0.9919    ]\n",
      "[0.         0.         0.         1.44477697]\n",
      "[0.         2.65698406 0.         0.        ]\n",
      "[1.96441881 3.43818406 0.         0.        ]\n",
      "[0.    0.    0.    0.084]\n",
      "[0.      0.      0.      3.51344]\n",
      "[ 2.55528806  4.44091872 -0.91        1.08976   ]\n",
      "[-0.7  0.   0.   0. ]\n"
     ]
    }
   ],
   "source": [
    "# As the question wanted, i print my q table:\n",
    "flag = False\n",
    "for i in packMan.qTable:\n",
    "    for j in i:\n",
    "        if(j!=0):\n",
    "            flag = True\n",
    "    if flag : print(i)\n",
    "    flag = False        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will change the init board and run the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'E' 'E' 'A' 'W']\n",
      " ['W' 'E' 'W' 'W' 'W' 'W']\n",
      " ['W' 'E' 'W' 'E' 'E' 'W']\n",
      " ['W' 'E' 'E' 'E' 'W' 'W']\n",
      " ['W' 'E' 'W' 'E' 'W' 'W']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W']]\n",
      "0.0395503044128418\n"
     ]
    }
   ],
   "source": [
    "newBoard = [\n",
    "            ['W', 'W', 'W', 'W', 'W', 'W'],\n",
    "            ['W', 'A', 'D', 'D', 'D', 'W'],\n",
    "            ['W', 'D', 'W', 'W', 'W', 'W'],\n",
    "            ['W', 'D', 'W', 'D', 'D', 'W'],\n",
    "            ['W', 'D', 'D', 'D', 'W', 'W'],\n",
    "            ['W', 'D', 'W', 'D', 'W', 'W'],\n",
    "            ['W', 'W', 'W', 'W', 'W', 'W']\n",
    "        ]\n",
    "\n",
    "packMan.initilaBoard = newBoard\n",
    "packMan.makeNewBoard()\n",
    "start = time.time()\n",
    "packMan.test(False)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 (Optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw in the test, when I give True to display parameter You can see animation of the game\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
